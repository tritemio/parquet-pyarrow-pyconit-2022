# Dataset setup

To generate the test parquet datasets run the bash script `create_datasets.sh`.
You shoud tweak the variables at the beginning of the script to choose
the batch size and number of batches.

The script `create_datasets.sh` calls 3 python scripts
in order to generate 3 versions of the same dataset:

1. raw version:  a flat folder of parquet files representing some sort of operational source data. 
   Each file is a single batch of raw data supposedly arriving in real time.
   Dataset generated by the python script `data_gen.py`.
   
2. ingested version: this version is generated from the raw data by processing each raw parquet file separately.
   It represents the dataset generated by the ingestion in real time as the raw data arrives.
   For this reason it is partitioned but not compacted (still has many files per partition).
   Dataset generated by the python script `ingestion.py`.
     
3. compacted version: a copy of the ingested dataset where files in each partition have been compacted 
   to a single parquet file.
   Dataset generated by the python script `compaction.py`.

# Python scripts

You can also call the python scripts individually. For details see the their help:

```bash
$ python data_gen.py --help
$ python ingestion.py --help
$ python compaction.py --help
```

# Notebooks

- **parquet-file-inspect.ipynb**: a notebook to inspect and display details of a **single parquet file**
- **parquet-dataset-inspect.ipynb**: a notebook to inspect and display details of **parquet dataset**
- **parquet-query-dataset.ipynb**: a notebook performing a dataset query
